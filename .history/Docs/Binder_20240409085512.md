# 进程间通信的方式
IPC 全称为 Inter Process Communication，意为进程间通信，是指两个进程间进行数据交换的过程。Linux 和 Android 都有各自的 IPC机制。

## Linux 进程间通信的方式
- 管道：在内存中创建一个共享文件，通信双方利用这个共享文件传递信息。这个共享文件只存在于内存中且不属于文件系统，另外管道属于半双工的通信方式，数据只能在一个方向上流动。
- 信号：它是一种在类Unix操作系统上用于进程间通信的方式，信号是异步的，用来告知一个进程发生了某个事件。在Linux系统中，信号是预定义的、可枚举的，并且通常与系统级事件关联。信号的目的主要是为了处理一些紧急的或特殊的情况，如中断、程序错误、外部终止请求等，举例：
    - SIGINT (编号 2): 用户从键盘生成中断（通常是按Ctrl+C）。
    - SIGTERM (编号 15): 请求终止一个进程，是一种温和的终止方式，允许程序进行清理操作。
    - SIGKILL (编号 9): 强制终止一个进程，该信号不能被捕获或忽略，确保进程终止。
    - SIGSEGV (编号 11): 当进程进行非法内存访问时发送。
    - SIGALRM (编号 14): 由alarm系统调用设置的定时器超时时发送。
- 信号量：常作为一种锁机制，它是一个计数器，用来控制多个进程对共享资源的访问，主要作为进程间或者线程间同步的手段。
- 消息队列：它允许一个或多个进程向队列中写入消息，并由一个或多个进程读取这些消息，信息需要复制两次。
- 共享内存：多个进程可以直接读写同一块内存空间，不需要复制，从而大幅度提高效率。
- 套接字：分为 Internet Socket 和 Unix Socket，前者用于网络通信，后者用于

## Android 进程间通信的方式

# Binder
这里总结一下 Android Binder 通信机制，主要参考《深入理解 Android 内核设计思想》以及《Android 进阶指北》，两者对源代码和流程做了详细的分析。

## Binder 通信原理

Binder 通信是基于**内存映射**实现的。Binder 驱动和提供服务的进程（binder_open、binder_mmap）的虚拟地址经过转换后(分段、分页)，指向的物理内存处于同一个位置，发送方进程通过`copy_from_user()`将某段数据复制到内核空间中，提供服务的进程可以直接访问到这段数据，Binder 驱动只用了一次复制，就实现了进程见的数据共享。

## 使用 Binder 的原因

- 性能：管道、消息队列、Unix Socket 都是复制两次，Binder 仅需复制一次，性能仅次于共享内存；
- 稳定性：Binder 是基于稳定的 C/S 架构设计的，这个架构通常采用两层架构，共享内存没有分层，难以控制；而且共享内存并发访问临界资源时，可能产生死锁；从稳定性角度讲，Binder 是优于共享内存的；
- 安全性：传统的 IPC 接收方无法获得发起方可靠的 UID/PID，无法鉴别对方的身份。Android 为每个安装好的应用分配了 UID，当访问某个服务的时候，通过 UID/PID 判断身份和权限，增强了系统安全性；
- 语言：Linux 是基于 C 语言的，C 语言是面向过程的。Android 中 Java/Kotlin 是面向对象的，而 Binder 更符合面向对象的思想，作为 Android 的通信机制更合适。

## 智能指针

C/C++中的指针问题犹如洪水猛兽：

- 指针没有初始化，指针异常导致系统宕机
- new 了对象之后没有及时 delete，导致内存泄露
- 野指针，当对象 delete 后，还要把 ptr 置为 null，否则有可能导致宕机

Android 中使用 `sp` 表示强制针，使用 `wp` 表示弱指针，增加了引用计数，并且解决了循环引用问题，避免了 C/C++ 中常见的指针问题。

## Parcel

Parcel 是进程间数据传输的载体，具有数据打包和重组的能力。它有 Java 层和 JNI 层的定义，但最终所有数据类型的读写操作都是通过本地代码实现的，Java 层只是持有了 JNI 层的指针`mNativePtr`。

使用 Parcel 要保证的一点是读写规则必须要一致，或者说写入方和读取方必须遵守相同的协议。

Parcel 除了读写基本数据，还可以读写 Binder，后面会介绍`writeStrongBinder`和`readStrongBinder`，很关键。

## Binder 驱动

Binder Driver 会将自己注册成一个 `misc device` ，并向上层提供一个`/dev/binder` 节点，它并不对应一个真实的硬件设备，可以理解为一个虚拟的文件系统。它提供了常用的 `file_operations`，比较关键的有以下几个：

- binder_open: 打开 binder 节点，为进程创建它自己的`binder_proc`(就是一块内存)实体并加入 Binder 全局管理中，之后进程对 Binder 设备的操作以它为基础；
- binder_mmap: 内存映射，Server 进程和 Binder 驱动指向的物理内存地址是同一个；
- binder_ioctl: 通过指令的方式处理了 Binder 驱动中的大部分业务；

## ProcessState&IPCThreadState

这两者是对使用 Binder 机制的封装，都是单例，是`native`类。
ProcessState 是进程共享的，主要做一些准备工作：

1. 负责打开 Binder 驱动、设置最大线程数量为 15;
2. 进行`mmap()`内存映射。

IPCThreadState 则存放于线程的`TLS`中，负责与 Binder 驱动进行具体的命令通信。
它有两个成员变量`mIn`和`mOut`，默认大小均为 256 个字节:

- 前者用来接收来自 Binder 驱动的数据；
- 后者用来存储发往 Binder 驱动的数据；

它最重要的函数是`transact()`，其中会调用`writeTransactionData`和`waitForResponse`:

- `writeTransactionData`将上层传过来的数据结构封装为`binder_transaction_data tr`结构体，并且将`handle`赋值给`tr.target.handle`，这个字段是用来标识目标的；
- `waitForResponse`开启一个主循环，调用`talkWithDriver`进一步调用`ioctl`与 Binder 驱动通信，直到收到 Server 的回复才跳出循环，将结果返回上层；

```
waitForResponse->talkWithDriver->binder_ioctl(Binder.cc)->binder_thread_read(Binder.cc)，线程挂起，进入睡眠等待状态。
```

## BpBinder&BBinder

这两个也是存在于`native`层，是 Binder 通信的双子星，都继承 `IBinder`。`BpBinder` 是客户端与服务端交互的代理类，而 `BBinder` 则代表了服务端，这两者是一一对应的，`BpBinder` 通过`handle`找到对应的 `BBinder`。

Java 层的 `BinderProxy`以及 native 层的 `BpBinder/BBinder` 都实现了对应的 `IBinder` 对象，`BinderProxy` 会持有 `BpBinder` 的指针，当 `Client` 发起跨进程调用的时候，数据大致流转过程如：

```
XxxServiceProxy->BinderProxy->BpBinder->IPCThreadState->Binder 驱动->BBinder->Binder->XxxService 对应的方法。
```

当从 `ServiceManager` 中查询到新的 `Server`，它对应的是 `BBinder`，它的内存引用会被 `Binder` 驱动映射为`handle`值（跨进程，同进程可以直接返回 `BBinder` 的内存地址）返回到 `Client`，这个时候会创建 `BpBinder(handle)`；

对于 native 来说，实现 `IInterface` 接口的是 `I##INTERFACE`（比如 IServiceManager），它是本地的代理，它的`interface_cast`方法其实就是定义在`IInterface.h`中的`I##INTERFACE`的`asInterface`，它的实现也是定义在`IInterface.h`中的`IMPLEMENT_MATE_INTERFACE(INTERFACE,NAME)`,这个方法中创建了`Bp#INTERFACE`，这个对象持有了`mRemote`--`BpBinder(handle)`。
`mRemote`来源于`BpRefBase`，`Bp#INTERFACE`继承了它，native 的对象关系总得来说如下：

```
//#INTERFACE定义在 IInterface.h 指服务名，如 ServiceManager
Bp#INTERFACE->BpInterface->BpRefBase 继承
Bp#INTERFACE->I#INTERFACE->IInterface 继承
BpBinder、BBinder->IBinder 继承
Bp#INTERFACE->BpBinder(handle) 持有
BpBinder(handle)->BBbinder 一对一(handle)通信
```

## ServiceManager

它是 Binder 大管家，类似于 `DNS` 服务器，本身也是一个 `Binder Server`。

### 初始化

它是 `init` 进程负责启动的，具体是在解析 `init.rc` 配置文件时启动的，Android7.0 后对 `init.rc` 配置文件进行了拆分，`ServiceManager` 启动的脚本在 `ServiceManager.rc` 中。
`ServiceManager` 是以 `system` 身份运行的，是系统中的关键服务，不会退出。如果退出了，系统会重启，`zygote`、`media`、`surfaceFlinger` 等都会重启。

### 启动

`servicemanager.c` 的 `main` 方法中主要做了三件事情:

1. 调用 `binder_open` 打开 `binder` 设备，并申请 128kb 的内存空间；
2. 调用 `binder_become_context_manager`，将 `servicemanager` 注册为 `binder` 机制的上下文管理者；
3. 调用 `binder_loop`，循环等待和处理客户端的请求；
   loop 是一种事件循环机制，使用`ioctl`从 `Binder` 驱动中读取数据，然后交给`binder_parse`函数处理，如果没有请求当前线程就会在 `binder` 驱动中睡眠。

### 注册服务

以`MediaPlayerService`为例，它注册服务的位置在 `mediaserver.cpp` 的 main 函数中:

```
//...
MediaPlayerService::instantiate();
//...
```

这个方法调用`defaultServiceManager()`获取 `IServiceManager`，其实是一个 `BpServiceManager`，它的`mRemote`指向`BpBinder(0)`：

```
addService(IServiceManager)->transact(BpBinder)->transact(IPCThreadState)
->writeTransactionData(BC_TRANSACTION)(IPCThreadState)->waitForResponse(IPCThreadState)->talkWithDriver(IPCThreadState)
->ioctl->binder_ioctl(Binder)->copy_from_user(Binder)->binder_transaction(Binder)
```

#### Binder 驱动执行指令

`binder_transaction`是 Binder 具体执行命令的地方，对于`BC_TRANSACTION`:

1. 根据 `handle` 获取目标对象对应的`target_node`，这里是`binder_context_mgr_node`，代表 target 是 ServiceManager;
2. 找出目标对象的`target_proc`和`target_thread`
3. 根据 `target_thread`获取`target_list`和`target_wait`，意为`todo`和`wait`
4. 生成一个`binder_transaction`变量，描述本次将要进行的`transaction`，加入到`todo`中，目标对象被唤醒时，从这个队列中取出任务
5. 生成`binder_work`，代表调用者线程有一个未完成的`transaction`
6. 构造上述`binder_transaction`信息，包括 t->from，t->to_proc/t->to_thread，transaction 相关的信息如 t->code，t->buffer
7. 申请到 t->buffer 后，调用`copy_from_user`把 tr->data.ptr.buffer 拷贝到 t-buffer 中，因为 t->buffer 指向的内存空间和目标对象是共享的，所以只需一次复制即可把数据从 Client 复制到 Server
8. 将 `todo` 和 `wait` 加入到目标对象对应的队列中，调用`wake_up_interruptible(target_wait)`唤醒目标

#### ServiceManager 执行任务

对于 Client 讲，如果需要结果，则进入阻塞状态等待 Server 的处理结果；对于 Server 来讲，上面提到 ServiceManager 初始化时会进入事件循环，此时会被唤醒，通过`ioctl`读取数据，通过`binder_parse`解析命令，这里的指令是`BR_TRANSACTION`,那么：

- 初始化 `reply`
- 通过`func`函数，也就是`svcmgr_handler`来具体处理用户请求
- 将处理结果通过 Binder 驱动发送给请求的客户端

`svcmgr_handler`主要分支如下：

```c++
    switch(txn->code) {
301    case SVC_MGR_GET_SERVICE:
302    case SVC_MGR_CHECK_SERVICE:
303        s = bio_get_string16(msg, &len);
304        if (s == NULL) {
305            return -1;
306        }
307        handle = do_find_service(s, len, txn->sender_euid, txn->sender_pid);
308        if (!handle)
309            break;
310        bio_put_ref(reply, handle);
311        return 0;
312
313    case SVC_MGR_ADD_SERVICE:
314        s = bio_get_string16(msg, &len);
315        if (s == NULL) {
316            return -1;
317        }
318        handle = bio_get_ref(msg);
319        allow_isolated = bio_get_uint32(msg) ? 1 : 0;
320        dumpsys_priority = bio_get_uint32(msg);
321        if (do_add_service(bs, s, len, handle, txn->sender_euid, allow_isolated, dumpsys_priority,
322                           txn->sender_pid))
323            return -1;
324        break;
325
326    case SVC_MGR_LIST_SERVICES: {
327        uint32_t n = bio_get_uint32(msg);
328        uint32_t req_dumpsys_priority = bio_get_uint32(msg);
329
330        if (!svc_can_list(txn->sender_pid, txn->sender_euid)) {
331            ALOGE("list_service() uid=%d - PERMISSION DENIED\n",
332                    txn->sender_euid);
333            return -1;
334        }
335        si = svclist;
336        // walk through the list of services n times skipping services that
337        // do not support the requested priority
338        while (si) {
339            if (si->dumpsys_priority & req_dumpsys_priority) {
340                if (n == 0) break;
341                n--;
342            }
343            si = si->next;
344        }
345        if (si) {
346            bio_put_string16(reply, si->name);
347            return 0;
348        }
349        return -1;
350    }
351    default:
352        ALOGE("unknown code %d\n", txn->code);
353        return -1;
354    }
```

我们可以看到，SVC_MSG_ADD_SERVICE 分支里面，首先通过`bio_get_ref`取到的是 handle 值:

```c++
    uint32_t bio_get_ref(struct binder_io *bio)
645{
646    struct flat_binder_object *obj;
647
648    obj = _bio_get_obj(bio);
649    if (!obj)
650        return 0;
651
652    if (obj->hdr.type == BINDER_TYPE_HANDLE)
653        return obj->handle;
654
655    return 0;
656}
```

然后通过`do_add_service`构造`svcinfo`，添加到链表头部：

```c++
    si = malloc(sizeof(*si) + (len + 1) * sizeof(uint16_t));
228        if (!si) {
229            ALOGE("add_service('%s',%x) uid=%d - OUT OF MEMORY\n",
230                 str8(s, len), handle, uid);
231            return -1;
232        }
233        si->handle = handle;
234        si->len = len;
235        memcpy(si->name, s, (len + 1) * sizeof(uint16_t));
236        si->name[len] = '\0';
237        si->death.func = (void*) svcinfo_death;
238        si->death.ptr = si;
239        si->allow_isolated = allow_isolated;
240        si->dumpsys_priority = dumpsys_priority;
241        si->next = svclist;
242        svclist = si;
```

也就是说，这里面保存的只有`handle`值，那就意味着，当调用`getService`查询服务的时候，也只能返回`handle`值，正如上边`SVC_MGR_CHECK_SERVICE`分支写的那样。

#### BBinder 的创建时机

那我们不禁好奇，这里的`handle`值是哪里来的？我们之前提到的`BBinder`怎么没有出现？
这要从服务端看起，一个服务通常是继承自 `Stub` 的，`Stub` 又继承自`Binder`类，`Binder`的构造方法中，调用了 native 的`getNativeBBinderHolder`方法：

```java
mObject = getNativeBBinderHolder();
```

```c++
    static jlong android_os_Binder_getNativeBBinderHolder(JNIEnv* env, jobject clazz)
897 {
898    JavaBBinderHolder* jbh = new JavaBBinderHolder();
899    return (jlong) jbh;
900 }
```

JavaBBinderHolder 是 JavaBBinder 的载体，提供 get 方法获取 JavaBBinder:

```c++
393 class JavaBBinderHolder
394 {
395 public:
396    sp<JavaBBinder> get(JNIEnv* env, jobject obj)
397    {
398        AutoMutex _l(mLock);
399        sp<JavaBBinder> b = mBinder.promote();
400        if (b == NULL) {
401            b = new JavaBBinder(env, obj);
402            mBinder = b;
403            ALOGV("Creating JavaBinder %p (refs %p) for Object %p, weakCount=%" PRId32 "\n",
404                 b.get(), b->getWeakRefs(), obj, b->getWeakRefs()->getWeakCount());
405        }
406
407        return b;
408    }
409
410    sp<JavaBBinder> getExisting()
411    {
412        AutoMutex _l(mLock);
413        return mBinder.promote();
414    }
415
416 private:
417    Mutex           mLock;
418    wp<JavaBBinder> mBinder;
419 };
```

在调用 `addService` 的时候，前面我们有一步没有提到，那就是在向 Parcel 写数据的时候，会调用`writeStrongBinder`将当前 Service 对象传入，它对应的 native 方法如下：

```c++
298 static void android_os_Parcel_writeStrongBinder(JNIEnv* env, jclass clazz, jlong nativePtr, jobject object)
299 {
300    Parcel* parcel = reinterpret_cast<Parcel*>(nativePtr);
301    if (parcel != NULL) {
302        const status_t err = parcel->writeStrongBinder(ibinderForJavaObject(env, object));
303        if (err != NO_ERROR) {
304            signalExceptionForError(env, clazz, err);
305        }
306    }
307 }
```

在`ibinderForJavaObject`这个方法中，通过 JNI 获取到了保存在 Binder 中的 JavaBBinderHolder 指针，真正调用了 JavaBBinderHolder 的 get 方法，创建了 JavaBBinder 对象：

```c++
    sp<IBinder> ibinderForJavaObject(JNIEnv* env, jobject obj)
691 {
692    if (obj == NULL) return NULL;
693
694    // Instance of Binder?
695    if (env->IsInstanceOf(obj, gBinderOffsets.mClass)) {
696        JavaBBinderHolder* jbh = (JavaBBinderHolder*)
697            env->GetLongField(obj, gBinderOffsets.mObject);
698        return jbh->get(env, obj);
699    }
700
701    // Instance of BinderProxy?
702    if (env->IsInstanceOf(obj, gBinderProxyOffsets.mClass)) {
703        return getBPNativeData(env, obj)->mObject;
704    }
705
706    ALOGW("ibinderForJavaObject: %p is not a Binder object", obj);
707    return NULL;
708 }
```

注意，JavaBBinder 是持有 Java 层 Binder 对象的引用的。这样，在`OnTransact`方法中，就可以把 Client 的请求通过 JNI 转发到 Java 的 Binder(`execTransact`)中。

在 `writeStrongBinder` 方法中，调用了`flatten_binder`的构造方法:

```c++
1082 status_t Parcel::writeStrongBinder(const sp<IBinder>& val)
1083 {
1084    return flatten_binder(ProcessState::self(), val, this);
1085 }
```

这个方法中主要是设置了对象的类型：

```c++
obj.hdr.type = BINDER_TYPE_BINDER;
obj.binder = reinterpret_cast<uintptr_t>(local->getWeakRefs());
obj.cookie = reinterpret_cast<uintptr_t>(local);
```

最终，一个 Binder 对象就被写入了 Parcel 中。

#### Binder 驱动保存 Binder 对象

Binder 驱动设计中，只要用户携带一个 Binder 对象路过它，就会被记录下来。`binder_get_node`会查找当前`proc->nodes`树中是否已经存在这个 Binder 对象，否则就会生成 `binder_node` 并加入到 nodes 树中。因此，属于当前 proc 的所有 Binder 对象在驱动中都是有记录的，比如当前 proc 也许是 SystemServer 进程。除此之外，还会维护`proc->refs_by_node`树，树中没有指向`binder_node`的节点则创建`binder_ref`节点，它里面记录了两个红黑树节点（ 分别加入 proc->refs_by_desc 和 proc->refs_by_node）、`binder_proc`、`binder_node`以及`desc`，其中`desc`就是一个自动增长的 `handle` 值，这样 `handle` 值就与 `BBinder` 具有了映射关系。

最终，将 `flat_binder_object`的 type 由初始的 `BINDER_TYPE_BINDER`改为`BINDER_TYPE_HANDLE`，并将 `binder_ref`的 `desc`赋值给 handle。

### 获取服务

```java
120    public IBinder getService(String name) throws RemoteException {
121        Parcel data = Parcel.obtain();
122        Parcel reply = Parcel.obtain();
123        data.writeInterfaceToken(IServiceManager.descriptor);
124        data.writeString(name);
125        mRemote.transact(GET_SERVICE_TRANSACTION, data, reply, 0);
126        IBinder binder = reply.readStrongBinder();
127        reply.recycle();
128        data.recycle();
129        return binder;
130    }
```

获取服务的过程和添加服务是类似的，由上可知，最终 Client 会拿到 BBinder 对应的 handle 值，逐层返回，创建 BpBinder(Handle)对象，关联为 I#INTERFACE 的`mRemote`，这些上边介绍`BpBinder&BBinder`时也有提到过。

有一点需要在这里分析，就是 BpBinder 是如何与 BinderProxy 关联的，关键的代码就是当进程阻塞返回后，调用了`reply.readStrongBinder()`，这是一个 native 调用：

```c++
// JNI 方法定义
static jobject android_os_Parcel_readStrongBinder(JNIEnv* env, jobject clazz, jlong nativePtr)
{
    Parcel* parcel = reinterpret_cast<Parcel*>(nativePtr);
    sp<IBinder> binder = parcel->readStrongBinder();//1
    return javaObjectForIBinder(env, binder);//2
}
```

1 的流程即为创建 BpBinder 的过程：
`parcel->readStrongBinder()`会调用`unflatten_binder`:

```c++
323 status_t unflatten_binder(const sp<ProcessState>& proc,
324    const Parcel& in, wp<IBinder>* out)
325 {
326    const flat_binder_object* flat = in.readObject(false);
327
328    if (flat) {
329        switch (flat->hdr.type) {
330            case BINDER_TYPE_BINDER:
331                *out = reinterpret_cast<IBinder*>(flat->cookie);
332                return finish_unflatten_binder(NULL, *flat, in);
333            case BINDER_TYPE_WEAK_BINDER:
334                if (flat->binder != 0) {
335                    out->set_object_and_refs(
336                        reinterpret_cast<IBinder*>(flat->cookie),
337                        reinterpret_cast<RefBase::weakref_type*>(flat->binder));
338                } else {
339                    *out = NULL;
340                }
341                return finish_unflatten_binder(NULL, *flat, in);
342            case BINDER_TYPE_HANDLE:
343            case BINDER_TYPE_WEAK_HANDLE:
344                *out = proc->getWeakProxyForHandle(flat->handle);
345                return finish_unflatten_binder(
346                    static_cast<BpBinder*>(out->unsafe_get()), *flat, in);
347        }
348    }
349    return BAD_TYPE;
350 }
```

最终调用`proc->getWeakProxyForHandle(flat->handle)`，这个方法中创建了`BpBinder`:

```c++
244 sp<IBinder> ProcessState::getStrongProxyForHandle(int32_t handle)
245 {
246    sp<IBinder> result;
248    AutoMutex _l(mLock);
250    handle_entry* e = lookupHandleLocked(handle);
252    if (e != NULL) {
256        IBinder* b = e->binder;
257        if (b == NULL || !e->refs->attemptIncWeak(this)) {
258            if (handle == 0) {
278                Parcel data;
279                status_t status = IPCThreadState::self()->transact(
280                        0, IBinder::PING_TRANSACTION, data, NULL, 0);
281                if (status == DEAD_OBJECT)
282                   return NULL;
283            }
284
285            b = BpBinder::create(handle);
286            e->binder = b;
287            if (b) e->refs = b->getWeakRefs();
288            result = b;
289        } else {
293            result.force_set(b);
294            e->refs->decWeak(this);
295        }
296    }
298    return result;
299 }
```

2 的流程即为创建 BinderProxy 并关联 BpBinder 的过程：

```c++
644 jobject javaObjectForIBinder(JNIEnv* env, const sp<IBinder>& val)
645 {
646    if (val == NULL) return NULL;
647
648    if (val->checkSubclass(&gBinderOffsets)) {
649        // It's a JavaBBinder created by ibinderForJavaObject. Already has Java object.
650        jobject object = static_cast<JavaBBinder*>(val.get())->object();
651        LOGDEATH("objectForBinder %p: it's our own %p!\n", val.get(), object);
652        return object;
653    }
654
655    // For the rest of the function we will hold this lock, to serialize
656    // looking/creation/destruction of Java proxies for native Binder proxies.
657    AutoMutex _l(gProxyLock);
658
659    BinderProxyNativeData* nativeData = gNativeDataCache;
660    if (nativeData == nullptr) {
661        nativeData = new BinderProxyNativeData();
662    }
663    // gNativeDataCache is now logically empty.
664    jobject object = env->CallStaticObjectMethod(gBinderProxyOffsets.mClass,
665            gBinderProxyOffsets.mGetInstance, (jlong) nativeData, (jlong) val.get());
666    if (env->ExceptionCheck()) {
667        // In the exception case, getInstance still took ownership of nativeData.
668        gNativeDataCache = nullptr;
669        return NULL;
670    }
671    BinderProxyNativeData* actualNativeData = getBPNativeData(env, object);
672    if (actualNativeData == nativeData) {
673        // New BinderProxy; we still have exclusive access.
674        nativeData->mOrgue = new DeathRecipientList;
675        nativeData->mObject = val;
676        gNativeDataCache = nullptr;
677        ++gNumProxies;
678        if (gNumProxies >= gProxiesWarned + PROXY_WARN_INTERVAL) {
679            ALOGW("Unexpectedly many live BinderProxies: %d\n", gNumProxies);
680            gProxiesWarned = gNumProxies;
681        }
682    } else {
683        // nativeData wasn't used. Reuse it the next time.
684        gNativeDataCache = nativeData;
685    }
686
687    return object;
688 }
```

我们看到这里用了缓存机制，但核心代码如下:

```c++
664 jobject object = env->CallStaticObjectMethod(gBinderProxyOffsets.mClass,
665            gBinderProxyOffsets.mGetInstance, (jlong) nativeData, (jlong) val.get());
```

`gBinderProxyOffsets`是 JNI 加载时预先保存的 Java 属性变量的引用，这行代码的意思就是创建 BinderProxy，并将指针指向 BpBinder。

`getService`返回后通会将 IBinder 转为业务逻辑接口，比如使用 AIDL 时：

```java
final IBinder b = ServiceManager.getService(Context.ACTIVITY_SERVICE);
final IActivityManager am = IActivityManager.Stub.asInterface(b);
```

`Stub`是 AIDL 自动生成的，类似于：

```java
public static IMyService asInterface(IBinder obj) {
    if (obj == null) {
        return null;
    }
    IInterface iin = obj.queryLocalInterface(IMyService.DESCRIPTOR);
    if ((iin != null) && (iin instanceof IMyService)) {
        return (IMyService)iin;
    }
    return new Proxy(obj);
}
```

- `queryLocalInterface(String descriptor)` 是 IBinder 的一个方法，它尝试查找一个与给定描述符匹配的本地接口实例。如果这个 IBinder 对象实际上是当前进程的一个 Binder 实例，那么它会返回一个直接的接口实现，这可以避免不必要的 IPC 调用，提高效率。
- `DESCRIPTOR` 是一个在 AIDL 文件中定义的唯一标识符，通常用于标识特定的服务接口。
- 如果 `queryLocalInterface` 返回 `null` 或返回的不是所需的服务接口实例，说明 IBinder 是一个远程代理。这时，需要创建一个新的代理实例 (Proxy)，这个代理实现了目标接口，并通过底层的 Binder 机制将调用转发到远程服务。
- Proxy 中持有了 BinderProxy 的实例，即 `mRemote`

当发起远程调用的时候，调用关系即: `XxxProxy->BinderProxy->BpBinder->BBinder->Binder`，这样，就从 client 到 server，从 Java 层到 native 层，完成了一次调用。

### 调用方法

当获取的代理服务对象(BpBinder(handle))后，就可以发起方法调用了。
以 SystemServer 进程为例，SystemServer 初始化的时候会创建一个主循环，和 ServiceManager 进程是类似的，不断调用`talkWithDriver`以及`executeCommand`。
SystemServer 虽然有多个 Service，但并不会打开多个 Binder 驱动，因为 IProcessState 是进程共享的，如何区分不同的 Service 呢？答案就是 `handle`。
当 Client 请求过来后，进程被唤醒，虽然 SystemServer 中包含很多服务，但是 handle 值区分了具体是哪一个 BBinder 响应请求，`executeCommand`中正是将请求委托给 BBinder 的`transact`方法处理， 根据上文可知，这里的 BBinder 其实是 JavaBBinder，它的`OnTransact`方法中通过 JNI 调用了 Java 层的 Binder 的`execTransact`，最终调用 Service 需要重写的`OnTransact`方法，如果是继承 AIDL 生成的 Stub 类，则只需要实现相应的业务处理接口即可。
方法执行完毕后，通过`replay`回调，将数据写入到 Binder 驱动缓冲区，进而`copy_to_user`到 `IPCThreadState` 的`mIn`中，唤醒 Client 线程，将结果层层返回。

## Java 层的实现

上边所有关于 Binder 的分析大多数都是基于 native 层的，它是 Binder 核心实现的地方，对于 Java 层，Binder 也提供了对应的 Api。
Java 层和 native 层要通过 JNI 进行通信，JNI 的注册是在 Zygote 进程启动过程中进行的。关于 Binder 的 JNI 注册，都定义在`android_util_Binder.cpp`中：

```c++
1427 int register_android_os_Binder(JNIEnv* env)
1428 {
1429    if (int_register_android_os_Binder(env) < 0)
1430        return -1;
1431    if (int_register_android_os_BinderInternal(env) < 0)
1432        return -1;
1433    if (int_register_android_os_BinderProxy(env) < 0)
1434        return -1;
1435    //...
1459 }
```

以 BinderProxy 为例，这里将必要的属性 id、方法 id 缓存到了`binderproxy_offsets_t`结构体中，目的是为了之后使用时提高效率：

```c++
1398 const char* const kBinderProxyPathName = "android/os/BinderProxy";
1399
1400 static int int_register_android_os_BinderProxy(JNIEnv* env)
1401 {
1402    jclass clazz = FindClassOrDie(env, "java/lang/Error");
1403    gErrorOffsets.mClass = MakeGlobalRefOrDie(env, clazz);
1404
1405    clazz = FindClassOrDie(env, kBinderProxyPathName);
1406    gBinderProxyOffsets.mClass = MakeGlobalRefOrDie(env, clazz);
1407    gBinderProxyOffsets.mGetInstance = GetStaticMethodIDOrDie(env, clazz, "getInstance",
1408            "(JJ)Landroid/os/BinderProxy;");
1409    gBinderProxyOffsets.mSendDeathNotice = GetStaticMethodIDOrDie(env, clazz, "sendDeathNotice",
1410            "(Landroid/os/IBinder$DeathRecipient;)V");
1411    gBinderProxyOffsets.mDumpProxyDebugInfo = GetStaticMethodIDOrDie(env, clazz, "dumpProxyDebugInfo",
1412            "()V");
1413    gBinderProxyOffsets.mNativeData = GetFieldIDOrDie(env, clazz, "mNativeData", "J");
1414
1415    clazz = FindClassOrDie(env, "java/lang/Class");
1416    gClassOffsets.mGetName = GetMethodIDOrDie(env, clazz, "getName", "()Ljava/lang/String;");
1417
1418    return RegisterMethodsOrDie(
1419        env, kBinderProxyPathName,
1420        gBinderProxyMethods, NELEM(gBinderProxyMethods));
1421 }
```

关于 Parcel 的 JNI 注册，定义在 `android_os_parcel.cpp`中：

```cpp
    const char* const kParcelPathName = "android/os/Parcel";
824
825 int register_android_os_Parcel(JNIEnv* env)
826 {
827    jclass clazz = FindClassOrDie(env, kParcelPathName);
828
829    gParcelOffsets.clazz = MakeGlobalRefOrDie(env, clazz);
830    gParcelOffsets.mNativePtr = GetFieldIDOrDie(env, clazz, "mNativePtr", "J");
831    gParcelOffsets.obtain = GetStaticMethodIDOrDie(env, clazz, "obtain", "()Landroid/os/Parcel;");
832    gParcelOffsets.recycle = GetMethodIDOrDie(env, clazz, "recycle", "()V");
833
834    return RegisterMethodsOrDie(env, kParcelPathName, gParcelMethods, NELEM(gParcelMethods));
835 }
```

`gParcelMethods`是一个二维数组，存放了 java 中的 native 方法名称、签名、native 方法引用。

Java 中对象有如下关系：

- Binder 和 BinderProxy 实现了 IBinder 接口，前者是服务端的代表，后者是客户端的代表
- BinderInternal 只在 Binder 框架中使用，其内部类 GCWatcher 用于处理 Binder 的垃圾回收
- Parcel 是一个数据包装类，可以在进程间传递，既可以传递基本数据类型又可以传递 Binder 对象；即有 Java 层的定义也有 native 层的定义，不过真正的实现还是在 native 中，Java 层会持有 native 层的指针。

#### 注册服务、获取服务

这两者上边都有提到过了，都是从 Java 层流转到 Native，不再赘述。

```java
    private static final Singleton<IActivityManager> IActivityManagerSingleton =
4130   new Singleton<IActivityManager>() {
4131        @Override
4132        protected IActivityManager create() {
4133              final IBinder b = ServiceManager.getService(Context.ACTIVITY_SERVICE);
4134              final IActivityManager am = IActivityManager.Stub.asInterface(b);
4135              return am;
4136        }
4137    };
```

```java
2722  ServiceManager.addService(Context.ACTIVITY_SERVICE, this, /* allowIsolated= */ true,
2723        DUMP_FLAG_PRIORITY_CRITICAL | DUMP_FLAG_PRIORITY_NORMAL | DUMP_FLAG_PROTO);
```

#### Java&Native

通过上述添加服务、获取服务的分析，我们也提到了 native 对象与 Java 对象是如何关联的，流程是怎样的，这里再贴出来：

```
Client: XxxServiceProxy(Java)->BinderProxy(Java)->BpBinder(Native)
Server: Binder(Java)->JavaBBinderHolder(Native); JavaBBinder(Native)->Binder(Java)
```

对于 Java 的继承关系：

```
Binder、BinderProxy->IBinder
//AIDL 中
IWindowManager.Stub.Proxy、IWindowManager.Stub->IWindowManager
```

对于 native 的继承关系：

```
//#INTERFACE定义在 IInterface.h 指服务名，如 ServiceManager
Bp#INTERFACE->BpInterface->BpRefBase 继承
Bp#INTERFACE->I#INTERFACE->IInterface 继承
BpBinder、BBinder->IBinder 继承
Bp#INTERFACE->BpBinder(handle) 持有
BpBinder(handle)->BBbinder 一对一(handle)通信
```

综上，我们可以知道，对于 Binder 而言，Java 层其实就是“冰山一角”，巨大的工作都是在 native 去实现的。
