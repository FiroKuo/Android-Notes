这里总结一下 Android Binder 通信机制，主要参考《深入理解 Android 内核设计思想》以及《Android 进阶指北》，两者对源代码和流程做了详细的分析。

# 进程间通信的方式

## Linux 进程间通信的方式

## Android 进程间通信的方式

# Binder

## Binder 通信原理

Binder 通信是基于**内存映射**实现的。Binder 驱动和提供服务的进程（binder_open、binder_mmap）的虚拟地址经过转换后(分段、分页)，指向的物理内存处于同一个位置，发送方进程通过`copy_from_user()`将某段数据复制到内核空间中，提供服务的进程可以直接访问到这段数据，Binder 驱动只用了一次复制，就实现了进程见的数据共享。

## 使用 Binder 的原因

- 性能：管道、消息队列、Unix Socket 都是复制两次，Binder 仅需复制一次，性能仅次于共享内存；
- 稳定性：Binder 是基于稳定的 C/S 架构设计的，这个架构通常采用两层架构，共享内存没有分层，难以控制；而且共享内存并发访问临界资源时，可能产生死锁；从稳定性角度讲，Binder 是优于共享内存的；
- 安全性：传统的 IPC 接收方无法获得发起方可靠的 UID/PID，无法鉴别对方的身份。Android 为每个安装好的应用分配了 UID，当访问某个服务的时候，通过 UID/PID 判断身份和权限，增强了系统安全性；
- 语言：Linux 是基于 C 语言的，C 语言是面向过程的。Android 中 Java/Kotlin 是面向对象的，而 Binder 更符合面向对象的思想，作为 Android 的通信机制更合适。

## 智能指针

C/C++中的指针问题犹如洪水猛兽：

- 指针没有初始化，指针异常导致系统宕机
- new 了对象之后没有及时 delete，导致内存泄露
- 野指针，当对象 delete 后，还要把 ptr 置为 null，否则有可能导致宕机

Android 中使用 `sp` 表示强制针，使用 `wp` 表示弱指针，增加了引用计数，解决了循环应用问题，避免了 C/C++ 中常见的指针问题。

## Parcel

Parcel 是进程间数据传输的载体，具有数据打包和重组的能力。它有 Java 层和 JNI 层的定义，但最终所有数据类型的读写操作都是通过本地代码实现的，Java 层只是持有了 JNI 层的指针`mNativePtr`。

使用 Parcel 要保证的一点是读写规则必须要一致，或者说写入方和读取方必须遵守相同的协议。

## Binder 驱动

Binder Driver 会将自己注册成一个 misc device ，并向上层提供一个/dev/binder 节点，它并不对应一个真实的硬件设备，可以理解为一个虚拟的文件系统。它提供了常用的 file_operations，比较关键的有以下几个：

- binder_open: 打开 binder 节点，为进程创建它自己的`binder_proc`(就是一块内存)实体并加入 Binder 全局管理中，之后进程对 Binder 设备的操作以它为基础；
- binder_mmap: 内存映射，Server 进程和 Binder 驱动指向的物理内存地址是同一个；
- binder_ioctl: 通过指令的方式处理了 Binder 驱动中的大部分业务；

## ProcessState&IPCThreadState

这两者是对使用 Binder 机制的封装，都是单例，是`native`类。
ProcessState 是进程共享的，主要做一些准备工作：

1. 负责打开 Binder 驱动、设置最大线程数量为 15;
2. 进行`mmap()`内存映射。

IPCThreadState 则存放于线程的`TLS`中，负责与 Binder 驱动进行具体的命令通信。
它有两个成员变量`mIn`和`mOut`，默认大小均为 256 个字节:

- 前者用来接收来自 Binder 驱动的数据；
- 后者用来存储发往 Binder 驱动的数据；

它最重要的函数是`transact()`，其中会调用`writeTransactionData`和`waitForResponse`:

- `writeTransactionData`将上层传过来的数据结构封装为`binder_transaction_data tr`结构体，并且将`handle`赋值给`tr.target.handle`，这个字段是用来标识目标的；
- `waitForResponse`开启一个主循环，调用`talkWithDriver`进一步调用`ioctl`与 Binder 驱动通信，直到收到 Server 的回复才跳出循环，将结果返回上层；
  waitForResponse->talkWithDriver->binder_ioctl(Binder.cc)->binder_thread_read(Binder.cc)，线程挂起，进入睡眠等待状态。

## BpBinder&BBinder

这两个也是存在于`native`层，是 Binder 通信的双子星，都继承 IBinder。BpBinder 是客户端与服务端交互的代理类，而 BBinder 则代表了服务端，这两者是一一对应的，BpBinder 通过`handle`找到对应的 BBinder。
Java 层的 `BinderProxy`以及 native 层的 `BpBinder/BBinder` 都实现了对应的 IBinder 对象，BinderProxy 会持有 BpBinder 的指针，当 Client 发起跨进程调用的时候，数据大致流转过程如： XxxServiceProxy->BinderProxy->BpBinder->IPCThreadState->Binder 驱动->BBinder->XxxService 对应的方法。

当从 ServiceManager 中查询到新的 Server，它对应的是 BBinder，它的内存引用会被 Binder 驱动映射为`handle`值（跨进程，同进程可以直接返回 BBinder 的内存地址）返回到 Client，这个时候会创建 BpBinder(handle)；

对于 native 来说，实现 `IInterface` 接口的是 `I##INTERFACE`（比如 IServiceManager），它是本地的代理，它的`interface_cast`方法其实就是定义在`IInterface.h`中的`I##INTERFACE`的`asInterface`，它的实现也是定义在`IInterface.h`中的`IMPLEMENT_MATE_INTERFACE(INTERFACE,NAME)`,这个方法中创建了`Bp#INTERFACE`，这个对象持有了`mRemote`--`BpBinder(handle)`。
`mRemote`来源于`BpRefBase`，`Bp#INTERFACE`继承了它，native 的对象关系总得来说如下：

```
//#INTERFACE定义在 IInterface.h 指服务名，如 ServiceManager
Bp#INTERFACE->BpInterface->BpRefBase 继承
Bp#INTERFACE->I#INTERFACE->IInterface 继承
BpBinder、BBinder->IBinder 继承
Bp#INTERFACE->BpBinder(handle) 持有
BpBinder(handle)->BBbinder 一对一(handle)通信
```

## ServiceManager

它是 Binder 大管家，类似于 DNS 服务器，本身也是一个 Binder Server。

### 初始化

它是 init 进程负责启动的，具体是在解析 init.rc 配置文件时启动的，Android7.0 后对 init.rc 配置文件进行了拆分，ServiceManager 启动的脚本在 ServiceManager.rc 中。
ServiceManager 是以 system 身份运行的，是系统中的关键服务，不会退出。如果退出了，系统会重启，zygote、media、surfaceFlinger 等都会重启。

### 启动

servicemanager.c 的 `main` 方法中主要做了三件事情:

1. 调用 binder_open 打开 binder 设备，并申请 128kb 的内存空间；
2. 调用 binder_become_context_manager，将 servicemanager 注册为 binder 机制的上下文管理者；
3. 调用 binder_loop，循环等待和处理客户端的请求；
   loop 是一种事件循环机制，使用`ioctl`从 Binder 驱动中读取数据，然后交给`binder_parse`函数处理，如果没有请求当前线程就会在 binder 驱动中睡眠。

### 注册服务

以`MediaPlayerService`为例，它注册服务的位置在 mediaserver.cpp 的 main 函数中:

```
//...
MediaPlayerService::instantiate();
//...
```

这个方法调用`defaultServiceManager()`获取 `IServiceManager`，其实是一个 `BpServiceManager`，它的`mRemote`指向`BpBinder(0)`：

```
addService(IServiceManager)->transact(BpBinder)->transact(IPCThreadState)
->writeTransactionData(BC_TRANSACTION)(IPCThreadState)->waitForResponse(IPCThreadState)->talkWithDriver(IPCThreadState)
->ioctl->binder_ioctl(Binder)->copy_from_user(Binder)->binder_transaction(Binder)
```
#### Binder驱动执行指令
`binder_transaction`是 Binder 具体执行命令的地方，对于`BC_TRANSACTION`:

1. 根据 `handle` 获取目标对象对应的`target_node`，这里是`binder_context_mgr_node`，代表 target 是 ServiceManager;
2. 找出目标对象的`target_proc`和`target_thread`
3. 根据 `target_thread`获取`target_list`和`target_wait`，意为`todo`和`wait`
4. 生成一个`binder_transaction`变量，描述本次将要进行的`transaction`，加入到`todo`中，目标对象被唤醒时，从这个队列中取出任务
5. 生成`binder_work`，代表调用者线程有一个未完成的`transaction`
6. 构造上述`binder_transaction`信息，包括 t->from，t->to_proc/t->to_thread，transaction 相关的信息如 t->code，t->buffer
7. 申请到 t->buffer 后，调用`copy_from_user`把 tr->data.ptr.buffer 拷贝到 t-buffer 中，因为 t->buffer 指向的内存空间和目标对象是共享的，所以只需一次复制即可把数据从 Client 复制到 Server
8. 将 `todo` 和 `wait` 加入到目标对象对应的队列中，调用`wake_up_interruptible(target_wait)`唤醒目标

#### ServiceManager执行任务
对于 Client 讲，如果需要结果，则进入阻塞状态等待 Server 的处理结果；对于 Server 来讲，上面提到 ServiceManager 初始化时会进入事件循环，此时会被唤醒，通过`ioctl`读取数据，通过`binder_parse`解析命令，这里的指令是`BR_TRANSACTION`,那么：

- 初始化 `reply`
- 通过`func`函数，也就是`svcmgr_handler`来具体处理用户请求
- 将处理结果通过 Binder 驱动发送给请求的客户端

`svcmgr_handler`主要分支如下：

```c++
    switch(txn->code) {
301    case SVC_MGR_GET_SERVICE:
302    case SVC_MGR_CHECK_SERVICE:
303        s = bio_get_string16(msg, &len);
304        if (s == NULL) {
305            return -1;
306        }
307        handle = do_find_service(s, len, txn->sender_euid, txn->sender_pid);
308        if (!handle)
309            break;
310        bio_put_ref(reply, handle);
311        return 0;
312
313    case SVC_MGR_ADD_SERVICE:
314        s = bio_get_string16(msg, &len);
315        if (s == NULL) {
316            return -1;
317        }
318        handle = bio_get_ref(msg);
319        allow_isolated = bio_get_uint32(msg) ? 1 : 0;
320        dumpsys_priority = bio_get_uint32(msg);
321        if (do_add_service(bs, s, len, handle, txn->sender_euid, allow_isolated, dumpsys_priority,
322                           txn->sender_pid))
323            return -1;
324        break;
325
326    case SVC_MGR_LIST_SERVICES: {
327        uint32_t n = bio_get_uint32(msg);
328        uint32_t req_dumpsys_priority = bio_get_uint32(msg);
329
330        if (!svc_can_list(txn->sender_pid, txn->sender_euid)) {
331            ALOGE("list_service() uid=%d - PERMISSION DENIED\n",
332                    txn->sender_euid);
333            return -1;
334        }
335        si = svclist;
336        // walk through the list of services n times skipping services that
337        // do not support the requested priority
338        while (si) {
339            if (si->dumpsys_priority & req_dumpsys_priority) {
340                if (n == 0) break;
341                n--;
342            }
343            si = si->next;
344        }
345        if (si) {
346            bio_put_string16(reply, si->name);
347            return 0;
348        }
349        return -1;
350    }
351    default:
352        ALOGE("unknown code %d\n", txn->code);
353        return -1;
354    }
```
我们可以看到，SVC_MSG_ADD_SERVICE 分支里面，首先通过`bio_get_ref`取到的是 handle 值:
```c++
    uint32_t bio_get_ref(struct binder_io *bio)
645{
646    struct flat_binder_object *obj;
647
648    obj = _bio_get_obj(bio);
649    if (!obj)
650        return 0;
651
652    if (obj->hdr.type == BINDER_TYPE_HANDLE)
653        return obj->handle;
654
655    return 0;
656}
```
然后通过`do_add_service`构造`svcinfo`，添加到链表头部：
```c++
    si = malloc(sizeof(*si) + (len + 1) * sizeof(uint16_t));
228        if (!si) {
229            ALOGE("add_service('%s',%x) uid=%d - OUT OF MEMORY\n",
230                 str8(s, len), handle, uid);
231            return -1;
232        }
233        si->handle = handle;
234        si->len = len;
235        memcpy(si->name, s, (len + 1) * sizeof(uint16_t));
236        si->name[len] = '\0';
237        si->death.func = (void*) svcinfo_death;
238        si->death.ptr = si;
239        si->allow_isolated = allow_isolated;
240        si->dumpsys_priority = dumpsys_priority;
241        si->next = svclist;
242        svclist = si;
```
也就是说，这里面保存的只有`handle`值，那就意味着，当调用`getService`查询服务的时候，也只能返回`handle`值，正如上边`SVC_MGR_CHECK_SERVICE`分支写的那样。

#### BBinder 的创建时机
那我们不禁好奇，这里的`handle`值是哪里来的？我们之前提到的`BBinder`怎么没有出现？
这要从服务端看起，一个服务通常是继承自 `Stub` 的，`Stub` 又继承自`Binder`类，`Binder`的构造方法中，调用了 native 的`getNativeBBinderHolder`方法：
```java
mObject = getNativeBBinderHolder();
```
```c++
    static jlong android_os_Binder_getNativeBBinderHolder(JNIEnv* env, jobject clazz)
897 {
898    JavaBBinderHolder* jbh = new JavaBBinderHolder();
899    return (jlong) jbh;
900 }
```
JavaBBinderHolder是 JavaBBinder 的载体，提供 get 方法获取 JavaBBinder:
```c++
393 class JavaBBinderHolder
394 {
395 public:
396    sp<JavaBBinder> get(JNIEnv* env, jobject obj)
397    {
398        AutoMutex _l(mLock);
399        sp<JavaBBinder> b = mBinder.promote();
400        if (b == NULL) {
401            b = new JavaBBinder(env, obj);
402            mBinder = b;
403            ALOGV("Creating JavaBinder %p (refs %p) for Object %p, weakCount=%" PRId32 "\n",
404                 b.get(), b->getWeakRefs(), obj, b->getWeakRefs()->getWeakCount());
405        }
406
407        return b;
408    }
409
410    sp<JavaBBinder> getExisting()
411    {
412        AutoMutex _l(mLock);
413        return mBinder.promote();
414    }
415
416 private:
417    Mutex           mLock;
418    wp<JavaBBinder> mBinder;
419 };
```

在调用 `addService` 的时候，前面我们有一步没有提到，那就是在向 Parcel 写数据的时候，会调用`writeStrongBinder`将当前 Service 对象传入，它对应的 native 方法如下：

```c++
298 static void android_os_Parcel_writeStrongBinder(JNIEnv* env, jclass clazz, jlong nativePtr, jobject object)
299 {
300    Parcel* parcel = reinterpret_cast<Parcel*>(nativePtr);
301    if (parcel != NULL) {
302        const status_t err = parcel->writeStrongBinder(ibinderForJavaObject(env, object));
303        if (err != NO_ERROR) {
304            signalExceptionForError(env, clazz, err);
305        }
306    }
307 }
```
在`ibinderForJavaObject`这个方法中，通过 JNI 获取到了保存在 Binder 中的 JavaBBinderHolder 指针，真正调用了 JavaBBinderHolder 的 get 方法，创建了 JavaBBinder 对象：
```c++
    sp<IBinder> ibinderForJavaObject(JNIEnv* env, jobject obj)
691 {
692    if (obj == NULL) return NULL;
693
694    // Instance of Binder?
695    if (env->IsInstanceOf(obj, gBinderOffsets.mClass)) {
696        JavaBBinderHolder* jbh = (JavaBBinderHolder*)
697            env->GetLongField(obj, gBinderOffsets.mObject);
698        return jbh->get(env, obj);
699    }
700
701    // Instance of BinderProxy?
702    if (env->IsInstanceOf(obj, gBinderProxyOffsets.mClass)) {
703        return getBPNativeData(env, obj)->mObject;
704    }
705
706    ALOGW("ibinderForJavaObject: %p is not a Binder object", obj);
707    return NULL;
708 }
```
在 `writeStrongBinder` 方法中，调用了`flatten_binder`的构造方法:
```c++
1082 status_t Parcel::writeStrongBinder(const sp<IBinder>& val)
1083 {
1084    return flatten_binder(ProcessState::self(), val, this);
1085 }
```
这个方法中主要是设置了对象的类型：
```c++
obj.hdr.type = BINDER_TYPE_BINDER;
obj.binder = reinterpret_cast<uintptr_t>(local->getWeakRefs());
obj.cookie = reinterpret_cast<uintptr_t>(local);
```
最终，一个 Binder 对象就被写入了 Parcel 中。

#### Binder驱动保存Binder对象
Binder 驱动设计中，只要用户携带一个 Binder 对象路过它，就会被记录下来。`binder_get_node`会查找当前`proc->nodes`树中是否已经存在这个 Binder 对象，否则就会生成 `binder_node` 并加入到 nodes 树中。因此，属于当前 proc 的所有 Binder 对象在驱动中都是有记录的，比如当前 proc 也许是 SystemServer 进程。除此之外，还会维护`proc->refs_by_node`树，树中没有指向`binder_node`的节点则创建`binder_ref`节点，它里面记录了两个红黑树节点（ 分别加入 proc->refs_by_desc 和 proc->refs_by_node）、`binder_proc`、`binder_node`以及`desc`，其中`desc`就是一个自动增长的 `handle` 值，这样 `handle` 值就与 `BBinder` 具有了映射关系。

最终，将 `flat_binder_object`的 type 由初始的 `BINDER_TYPE_BINDER`改为`BINDER_TYPE_HANDLE`，并将 `binder_ref`的 `desc`赋值给 handle。

### 获取服务
```java
120    public IBinder getService(String name) throws RemoteException {
121        Parcel data = Parcel.obtain();
122        Parcel reply = Parcel.obtain();
123        data.writeInterfaceToken(IServiceManager.descriptor);
124        data.writeString(name);
125        mRemote.transact(GET_SERVICE_TRANSACTION, data, reply, 0);
126        IBinder binder = reply.readStrongBinder();
127        reply.recycle();
128        data.recycle();
129        return binder;
130    }
```
获取服务的过程和添加服务是类似的，由上可知，最终 Client 会拿到 BBinder 对应的 handle 值，逐层返回，创建 BpBinder(Handle)对象，关联为 I#INTERFACE的`mRemote`，这些上边介绍`BpBinder&BBinder`时也有提到过。

有一点需要在这里分析，就是 BpBinder 是如何与 BinderProxy 关联的，关键的代码就是当进程阻塞返回后，调用了`reply.readStrongBinder()`，这是一个 native 调用：
```c++
// JNI 方法定义
static jobject android_os_Parcel_readStrongBinder(JNIEnv* env, jobject clazz, jlong nativePtr)
{
    Parcel* parcel = reinterpret_cast<Parcel*>(nativePtr);
    sp<IBinder> binder = parcel->readStrongBinder();
    return javaObjectForIBinder(env, binder);
}
```
`parcel->readStrongBinder()`会调用`unflatten_`

### 调用方法
当获取的代理服务对象(BpBinder(handle))后，就可以发起方法调用了。
以 SystemServer 进程为例，SystemServer 初始化的时候会创建一个主循环，和 ServiceManager 进程是类似的，不断调用`talkWithDriver`以及`executeCommand`。
SystemServer 虽然有多个 Service，但并不会打开多个 Binder 驱动，因为 IProcessState 是进程共享的，如何区分不同的 Service 呢？答案就是 `handle`。
当 Client 请求过来后，进程被唤醒，虽然 SystemServer 中包含很多服务，但是handle值区分了具体是哪一个 BBinder 响应请求，`executeCommand`中正是将请求委托给 BBinder 的`transact`方法处理， 根据上文可知，这里的 BBinder 其实是 JavaBBinder，它的`OnTransact`方法中通过 JNI 调用了 Java 层的 Binder 的`execTransact`，最终调用 Service 需要重写的`OnTransact`方法，如果是继承 AIDL 生成的 Stub 类，则只需要实现相应的业务处理接口即可。
方法执行完毕后，通过`replay`回调，将数据写入到 Binder 驱动缓冲区，进而`copy_to_user`到 `IPCThreadState` 的`mIn`中，唤醒 Client 线程，将结果层层返回。

## Java层的实现
上边所有关于 Binder 的分析都是基于 native 层的，它是 Binder 核心实现的地方，对于 Java 层，Binder 也提供了对应的 Api。
Java 层和 native 层要通过 JNI 进行通信，JNI 的注册是在 Zygote 进程启动过程中进行的。关于 Binder 的 JNI 注册，都定义在`android_util_Binder.cpp`中：
```c++
1427 int register_android_os_Binder(JNIEnv* env)
1428 {
1429    if (int_register_android_os_Binder(env) < 0)
1430        return -1;
1431    if (int_register_android_os_BinderInternal(env) < 0)
1432        return -1;
1433    if (int_register_android_os_BinderProxy(env) < 0)
1434        return -1;
1435    //...
1459 }
```
以 BinderProxy 为例，这里将必要的属性id、方法id缓存到了`binderproxy_offsets_t`结构体中，目的是为了之后使用时提高效率：
```c++
1398 const char* const kBinderProxyPathName = "android/os/BinderProxy";
1399
1400 static int int_register_android_os_BinderProxy(JNIEnv* env)
1401 {
1402    jclass clazz = FindClassOrDie(env, "java/lang/Error");
1403    gErrorOffsets.mClass = MakeGlobalRefOrDie(env, clazz);
1404
1405    clazz = FindClassOrDie(env, kBinderProxyPathName);
1406    gBinderProxyOffsets.mClass = MakeGlobalRefOrDie(env, clazz);
1407    gBinderProxyOffsets.mGetInstance = GetStaticMethodIDOrDie(env, clazz, "getInstance",
1408            "(JJ)Landroid/os/BinderProxy;");
1409    gBinderProxyOffsets.mSendDeathNotice = GetStaticMethodIDOrDie(env, clazz, "sendDeathNotice",
1410            "(Landroid/os/IBinder$DeathRecipient;)V");
1411    gBinderProxyOffsets.mDumpProxyDebugInfo = GetStaticMethodIDOrDie(env, clazz, "dumpProxyDebugInfo",
1412            "()V");
1413    gBinderProxyOffsets.mNativeData = GetFieldIDOrDie(env, clazz, "mNativeData", "J");
1414
1415    clazz = FindClassOrDie(env, "java/lang/Class");
1416    gClassOffsets.mGetName = GetMethodIDOrDie(env, clazz, "getName", "()Ljava/lang/String;");
1417
1418    return RegisterMethodsOrDie(
1419        env, kBinderProxyPathName,
1420        gBinderProxyMethods, NELEM(gBinderProxyMethods));
1421 }
```
关于 Parcel 的 JNI 注册，定义在 `android_os_parcel.cpp`中：
```cpp
    const char* const kParcelPathName = "android/os/Parcel";
824
825 int register_android_os_Parcel(JNIEnv* env)
826 {
827    jclass clazz = FindClassOrDie(env, kParcelPathName);
828
829    gParcelOffsets.clazz = MakeGlobalRefOrDie(env, clazz);
830    gParcelOffsets.mNativePtr = GetFieldIDOrDie(env, clazz, "mNativePtr", "J");
831    gParcelOffsets.obtain = GetStaticMethodIDOrDie(env, clazz, "obtain", "()Landroid/os/Parcel;");
832    gParcelOffsets.recycle = GetMethodIDOrDie(env, clazz, "recycle", "()V");
833
834    return RegisterMethodsOrDie(env, kParcelPathName, gParcelMethods, NELEM(gParcelMethods));
835 }
```
`gParcelMethods`是一个二维数组，存放了 java 中的 native 方法名称、签名、native 方法引用。

Java 中对象有如下关系：
- Binder 和 BinderProxy 实现了 IBinder 接口，前者是服务端的代表，后者是客户端的代表
- BinderInternal 只在 Binder 框架中使用，其内部类 GCWatcher 用于处理 Binder 的垃圾回收
- Parcel 是一个数据包装类，可以在进程间传递，既可以传递基本数据类型又可以传递 Binder 对象；即有 Java 层的定义也有 native 层的定义，不过真正的实现还是在 native 中，Java 层会持有 native 层的指针。

#### BpBinder&BinderProxy
native 的对象 BpBinder 是如何与 BinderProxy 产生联系的？上边分析获取服务的时候没有分析返回的场景，getService 如下